# ğŸ“š Streamlit RAG Kit

A production-ready, modular Retrieval-Augmented Generation (RAG) application powered by **Meta Llama 3.1** via Hugging Face, built with Streamlit.

[![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=flat&logo=streamlit&logoColor=white)](https://document-search-template.streamlit.app/)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âœ¨ Features

- ğŸš€ **Production-Ready**: Modular architecture with separation of concerns
- ğŸ§  **Powered by Meta Llama 3.1**: Open source LLM via Hugging Face Inference API
- ğŸ”„ **Automatic Fallback**: Gracefully handles deprecated models with fallback support
- ğŸ“ **Multi-Format Support**: PDF, DOCX, TXT, CSV, XLSX, JSON
- ğŸ” **Hybrid Search**: Combines vector similarity and BM25 keyword search
- ğŸ’¬ **Streaming Responses**: Real-time response generation
- ğŸ“Š **Source Citations**: View which documents informed each answer
- ğŸ¨ **Clean UI**: Intuitive Streamlit interface
- ğŸ”§ **Highly Configurable**: Adjust chunk size, search mode, and LLM parameters
- ğŸ“¦ **Easy Deployment**: Deploy to Streamlit Cloud, Docker, or any cloud platform

## ğŸ—ï¸ Architecture

```
streamlit-rag-kit/
â”œâ”€â”€ ğŸ¯ app.py                          # Main Streamlit app
â”œâ”€â”€ ğŸ“‹ requirements.txt                # Dependencies
â”œâ”€â”€ ğŸ“ components/                     # UI Components
â”‚   â”œâ”€â”€ file_uploader.py              # Document upload UI
â”‚   â”œâ”€â”€ chat_interface.py             # Chat UI
â”‚   â”œâ”€â”€ settings_panel.py             # Configuration panel
â”‚   â””â”€â”€ citation_viewer.py            # Source display
â”œâ”€â”€ ğŸ“ core/                           # Core RAG Engine
â”‚   â”œâ”€â”€ vector_stores/                # Vector DB implementations
â”‚   â”‚   â”œâ”€â”€ base.py                   # Base interface
â”‚   â”‚   â””â”€â”€ chroma.py                 # ChromaDB (default)
â”‚   â”œâ”€â”€ llm_providers/                # LLM integrations
â”‚   â”‚   â”œâ”€â”€ base.py                   # Base interface
â”‚   â”‚   â”œâ”€â”€ huggingface_provider.py   # Hugging Face (default)
â”‚   â”‚   â”œâ”€â”€ openai_provider.py        # OpenAI support
â”‚   â”‚   â”œâ”€â”€ anthropic_provider.py     # Anthropic support
â”‚   â”‚   â””â”€â”€ factory.py                # Provider factory
â”‚   â”œâ”€â”€ document_processor.py         # Document loading & chunking
â”‚   â””â”€â”€ retrieval_engine.py           # Main RAG pipeline
â”œâ”€â”€ ğŸ“ prompts/                        # Prompt templates
â”‚   â”œâ”€â”€ default_system.txt
â”‚   â”œâ”€â”€ detailed_analysis.txt
â”‚   â””â”€â”€ concise_summary.txt
â”œâ”€â”€ ğŸ“ utils/                          # Utilities
â”‚   â”œâ”€â”€ auth.py                       # API key management
â”‚   â”œâ”€â”€ session_state.py              # State management
â”‚   â”œâ”€â”€ cost_tracking.py              # Usage tracking
â”‚   â””â”€â”€ bm25_search.py                # BM25 implementation
â”œâ”€â”€ ğŸ“ deployment/                     # Deployment configs
â”‚   â”œâ”€â”€ streamlit_cloud_guide.md
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ ğŸ“ docs/                           # Documentation
â”‚   â”œâ”€â”€ quick_start.md
â”‚   â””â”€â”€ api_key_setup.md
â””â”€â”€ ğŸ“ .streamlit/
    â”œâ”€â”€ config.toml                   # App configuration
    â””â”€â”€ secrets.toml.example          # API key template
```

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd streamlit-rag-kit

# Install dependencies
pip install -r requirements.txt
```

### 2. Set Up API Key

Create `.streamlit/secrets.toml`:

```toml
HF_API_KEY = "hf_your-huggingface-api-key-here"
```

Get your API key from [Hugging Face Settings](https://huggingface.co/settings/tokens).

### 3. Run the App

```bash
streamlit run app.py
```

The app will open at `http://localhost:8501`.

### 4. Use the App

1. **Initialize Pipeline**: Click "Initialize Pipeline" in the sidebar
2. **Upload Documents**: Upload your files (PDF, DOCX, TXT, etc.)
3. **Ask Questions**: Start chatting with your documents!

## ğŸ“– Documentation

- [ğŸ“š Quick Start Guide](docs/quick_start.md) - Get started in 5 minutes
- [ğŸ”‘ API Key Setup](docs/api_key_setup.md) - Configure API keys
- [ğŸš€ Deployment Guide](deployment/streamlit_cloud_guide.md) - Deploy to production

## Project Structure

```
rag-template/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py       # Package exports
â”‚   â”œâ”€â”€ document_loader.py # Multi-format document loading
â”‚   â”œâ”€â”€ text_splitter.py   # Text chunking utilities
â”‚   â”œâ”€â”€ vector_store.py    # ChromaDB integration
â”‚   â”œâ”€â”€ bm25_search.py     # BM25 search implementation
â”‚   â”œâ”€â”€ llm.py            # LLM client (OpenAI/Anthropic)
â”‚   â””â”€â”€ rag_pipeline.py    # Main RAG pipeline
â””â”€â”€ data/                 # Data directory (created on init)
```

## Programmatic Usage

You can also use the RAG pipeline programmatically:

```python
from utils import RAGPipeline
from utils.rag_pipeline import SearchMode

# Initialize the pipeline
pipeline = RAGPipeline(
    llm_provider="huggingface",
    llm_api_key="your-hf-api-key",
    llm_model="meta-llama/Meta-Llama-3.1-8B-Instruct",
    search_mode=SearchMode.HYBRID,
    chunk_size=1000,
    chunk_overlap=200,
)

# Add documents
result = pipeline.add_documents(file_paths=["document.pdf", "data.csv"])
print(f"Added {result['chunks']} chunks from {result['documents']} documents")

# Query the pipeline
response = pipeline.query("What is the main topic of the documents?")
print(response.answer)

# Access sources
for source in response.sources:
    print(f"Source: {source['metadata']['source']}")
```

## Search Modes

### Vector Search (Semantic)
Uses sentence transformers to create embeddings and ChromaDB to find semantically similar content. Best for:
- Finding conceptually related content
- Handling paraphrased queries
- Understanding context and meaning

### BM25 Search (Keyword)
Traditional keyword-based search using term frequency. Best for:
- Exact term matching
- Technical terminology
- Specific names or codes

### Hybrid Search (Recommended)
Combines both methods using Reciprocal Rank Fusion. Benefits:
- Leverages strengths of both approaches
- More robust retrieval
- Better coverage of different query types

## Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `chunk_size` | 1000 | Maximum characters per chunk |
| `chunk_overlap` | 200 | Overlap between consecutive chunks |
| `n_results` | 5 | Number of chunks to retrieve |
| `temperature` | 0.7 | LLM response randomness (0-1) |
| `max_tokens` | 1000 | Maximum response length |
| `embedding_model` | all-MiniLM-L6-v2 | Sentence transformer model |

## Dependencies

- **streamlit**: Web application framework
- **chromadb**: Vector database for embeddings
- **sentence-transformers**: Text embeddings
- **rank-bm25**: BM25 search implementation
- **pypdf**: PDF parsing
- **python-docx**: Word document parsing
- **openpyxl**: Excel file parsing
- **pandas**: Data manipulation
- **huggingface_hub**: Hugging Face Inference API (default)
- **openai**: OpenAI API client (optional)
- **anthropic**: Anthropic API client (optional)

## ğŸ¯ Use Cases

- ğŸ“„ **Document Q&A**: Ask questions about your documents
- ğŸ” **Research Assistant**: Search through research papers
- ğŸ“š **Knowledge Base**: Build a searchable knowledge base
- ğŸ’¼ **Business Intelligence**: Query business documents
- ğŸ“– **Study Aid**: Interact with textbooks and notes

## ğŸš€ Deployment

### Streamlit Cloud (Easiest)

1. Push to GitHub
2. Connect to [Streamlit Cloud](https://share.streamlit.io)
3. Add API key in Secrets
4. Deploy!

See [Deployment Guide](deployment/streamlit_cloud_guide.md) for details.

### Docker

```bash
docker-compose up
```

## ğŸ“ License

MIT License

---

Made with â¤ï¸ using Meta Llama 3.1 via Hugging Face
